{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNTest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhi8nKDnEUCJOKOIfw/loq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PloyTara/ANNTest/blob/main/ANNTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc1MDEKINez-"
      },
      "source": [
        "# import Python Libraries \r\n",
        "import numpy as np \r\n",
        "from matplotlib import pyplot as plt \r\n",
        "\r\n",
        "# Sigmoid Function \r\n",
        "def sigmoid(z): \r\n",
        "\treturn 1 / (1 + np.exp(-z)) \r\n",
        "\r\n",
        "# Initialization of the neural network parameters \r\n",
        "# Initialized all the weights in the range of between 0 and 1 \r\n",
        "# Bias values are initialized to 0 \r\n",
        "def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures): \r\n",
        "\tW1 = np.random.randn(neuronsInHiddenLayers, inputFeatures) \r\n",
        "\tW2 = np.random.randn(outputFeatures, neuronsInHiddenLayers) \r\n",
        "\tb1 = np.zeros((neuronsInHiddenLayers, 1)) \r\n",
        "\tb2 = np.zeros((outputFeatures, 1)) \r\n",
        "\t\r\n",
        "\tparameters = {\"W1\" : W1, \"b1\": b1, \r\n",
        "\t\t\t\t\"W2\" : W2, \"b2\": b2} \r\n",
        "\treturn parameters \r\n",
        "\r\n",
        "# Forward Propagation \r\n",
        "def forwardPropagation(X, Y, parameters): \r\n",
        "\tm = X.shape[1] \r\n",
        "\tW1 = parameters[\"W1\"] \r\n",
        "\tW2 = parameters[\"W2\"] \r\n",
        "\tb1 = parameters[\"b1\"] \r\n",
        "\tb2 = parameters[\"b2\"] \r\n",
        "\r\n",
        "\tZ1 = np.dot(W1, X) + b1 \r\n",
        "\tA1 = sigmoid(Z1) \r\n",
        "\tZ2 = np.dot(W2, A1) + b2 \r\n",
        "\tA2 = sigmoid(Z2) \r\n",
        "\r\n",
        "\tcache = (Z1, A1, W1, b1, Z2, A2, W2, b2) \r\n",
        "\tlogprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y)) \r\n",
        "\tcost = -np.sum(logprobs) / m \r\n",
        "\treturn cost, cache, A2 \r\n",
        "\r\n",
        "# Backward Propagation \r\n",
        "def backwardPropagation(X, Y, cache): \r\n",
        "\tm = X.shape[1] \r\n",
        "\t(Z1, A1, W1, b1, Z2, A2, W2, b2) = cache \r\n",
        "\t\r\n",
        "\tdZ2 = A2 - Y \r\n",
        "\tdW2 = np.dot(dZ2, A1.T) / m \r\n",
        "\tdb2 = np.sum(dZ2, axis = 1, keepdims = True) \r\n",
        "\t\r\n",
        "\tdA1 = np.dot(W2.T, dZ2) \r\n",
        "\tdZ1 = np.multiply(dA1, A1 * (1- A1)) \r\n",
        "\tdW1 = np.dot(dZ1, X.T) / m \r\n",
        "\tdb1 = np.sum(dZ1, axis = 1, keepdims = True) / m \r\n",
        "\t\r\n",
        "\tgradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \r\n",
        "\t\t\t\t\"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1} \r\n",
        "\treturn gradients \r\n",
        "\r\n",
        "# Updating the weights based on the negative gradients \r\n",
        "def updateParameters(parameters, gradients, learningRate): \r\n",
        "\tparameters[\"W1\"] = parameters[\"W1\"] - learningRate * gradients[\"dW1\"] \r\n",
        "\tparameters[\"W2\"] = parameters[\"W2\"] - learningRate * gradients[\"dW2\"] \r\n",
        "\tparameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"] \r\n",
        "\tparameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"] \r\n",
        "\treturn parameters \r\n",
        "\r\n",
        "# Model to learn the XOR truth table \r\n",
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # XOR input \r\n",
        "Y = np.array([[0, 1, 1, 0]]) # XOR output \r\n",
        "\r\n",
        "# Define model parameters \r\n",
        "neuronsInHiddenLayers = 2 # number of hidden layer neurons (2) \r\n",
        "inputFeatures = X.shape[0] # number of input features (2) \r\n",
        "outputFeatures = Y.shape[0] # number of output features (1) \r\n",
        "parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures) \r\n",
        "epoch = 100000\r\n",
        "learningRate = 0.01\r\n",
        "losses = np.zeros((epoch, 1)) \r\n",
        "\r\n",
        "for i in range(epoch): \r\n",
        "\tlosses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters) \r\n",
        "\tgradients = backwardPropagation(X, Y, cache) \r\n",
        "\tparameters = updateParameters(parameters, gradients, learningRate) \r\n",
        "\r\n",
        "# Evaluating the performance \r\n",
        "plt.figure() \r\n",
        "plt.plot(losses) \r\n",
        "plt.xlabel(\"EPOCHS\") \r\n",
        "plt.ylabel(\"Loss value\") \r\n",
        "plt.show() \r\n",
        "\r\n",
        "# Testing \r\n",
        "X = np.array([[1, 1, 0, 0], [0, 1, 0, 1]]) # XOR input \r\n",
        "cost, _, A2 = forwardPropagation(X, Y, parameters) \r\n",
        "prediction = (A2 > 0.5) * 1.0\r\n",
        "# print(A2) \r\n",
        "print(prediction) \r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}